{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load and Parse Dataset"
      ],
      "metadata": {
        "id": "xUeZPZd68vGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod='ViT-B/32'"
      ],
      "metadata": {
        "id": "APh9wBsmQbiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"pata_clip_agen_b32.xlsx\""
      ],
      "metadata": {
        "id": "Q5MYcLZzPubc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download images"
      ],
      "metadata": {
        "id": "PV-B6PPW822L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time"
      ],
      "metadata": {
        "id": "2JpLq6zQRcC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load Images"
      ],
      "metadata": {
        "id": "SjSan0XT9Bgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYUJP2j8GWG5",
        "outputId": "b37bed07-256d-4716-8570-945d4c0a9fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "lfZ5noC3RfXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "X15voKv7WBWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/Pata_p/processed_dataset_with_images.json', 'r') as f:\n",
        "    processed_dataset= json.load(f)\n",
        "# Images are already saved and referenced in `image_path` keys\n"
      ],
      "metadata": {
        "id": "kRkg8FpwQvOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "u7LBdvNwF4sB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1b2b9d6-9c41-4fe0-99c1-530df8511bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP for attribute prediction"
      ],
      "metadata": {
        "id": "UH97c88M9N6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Use CLIP to Predict Attributes\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import torch\n",
        "import clip\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(mod, device=device)\n",
        "\n",
        "# Define attribute prompts\n",
        "age_prompts = {\n",
        "    'young': 'a photo of a young person',\n",
        "    'middleage': 'a photo of a middle-aged person',\n",
        "    'old': 'a photo of an old person'\n",
        "}\n",
        "\n",
        "gender_prompts = {\n",
        "    'male': 'a photo of a man',\n",
        "    'female': 'a photo of a woman'\n",
        "}\n",
        "\n",
        "race_prompts = {\n",
        "    'eastasian': 'a photo of an East Asian person',\n",
        "    'indian': 'a photo of an Indian person',\n",
        "    'black': 'a photo of a Black person',\n",
        "    'caucasian': 'a photo of a White person',\n",
        "    'latino': 'a photo of a Latino person',\n",
        "    'southeastasian': 'a photo of a Southeast Asian person',\n",
        "    'middleeastern': 'a photo of a Middle Eastern person',\n",
        "    'hispanic':'a photo of a hispanic person',\n",
        "}\n",
        "\n",
        "# Ensure all labels are in the prompts\n",
        "age_labels = set([entry['age'] for entry in processed_dataset])\n",
        "gender_labels = set([entry['gender'] for entry in processed_dataset])\n",
        "race_labels = set([entry['race'] for entry in processed_dataset])\n",
        "\n",
        "missing_age_labels = age_labels - set(age_prompts.keys())\n",
        "missing_gender_labels = gender_labels - set(gender_prompts.keys())\n",
        "missing_race_labels = race_labels - set(race_prompts.keys())\n",
        "\n",
        "if missing_age_labels:\n",
        "    print(f\"Missing age labels in prompts: {missing_age_labels}\")\n",
        "if missing_gender_labels:\n",
        "    print(f\"Missing gender labels in prompts: {missing_gender_labels}\")\n",
        "if missing_race_labels:\n",
        "    print(f\"Missing race labels in prompts: {missing_race_labels}\")\n",
        "\n",
        "# Tokenize prompts\n",
        "age_texts = [age_prompts[label] for label in age_prompts]\n",
        "gender_texts = [gender_prompts[label] for label in gender_prompts]\n",
        "race_texts = [race_prompts[label] for label in race_prompts]\n",
        "\n",
        "age_tokens = clip.tokenize(age_texts).to(device)\n",
        "gender_tokens = clip.tokenize(gender_texts).to(device)\n",
        "race_tokens = clip.tokenize(race_texts).to(device)\n"
      ],
      "metadata": {
        "id": "WlxamLow9Ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57da51b9-2187-47cb-d71d-b4752ee4fe6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-tnsurcf7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-tnsurcf7\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 88.0MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predict Attributes"
      ],
      "metadata": {
        "id": "jE7qblmd9Uxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, entry in enumerate(processed_dataset[:5]):  # Inspect the first 5 entries\n",
        "    print(f\"Entry {i}: {entry}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Uw71DM4SMZs",
        "outputId": "a8d27531-a2bb-40d7-a3fc-cb4040199cc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry 0: {'scene': 'forest', 'race': 'indian', 'gender': 'male', 'age': 'young', 'image_url': 'https://www.sbcltr.in/wp-content/uploads/2018/04/Shahzad-Qureshi-1020x520-942x480.jpg', 'filepath': '/content/drive/MyDrive/pata_images/0_forest_indian_male_young.jpg', 'image_path': '/content/drive/MyDrive/Pata_p/image_0.jpg'}\n",
            "Entry 1: {'scene': 'forest', 'race': 'indian', 'gender': 'male', 'age': 'young', 'image_url': 'https://akm-img-a-in.tosshub.com/indiatoday/images/bodyeditor/201808/jadav-647x499.jpg?VCE29UxbeCzCbfNOhkDfOEEM45Mhd54_', 'filepath': '/content/drive/MyDrive/pata_images/1_forest_indian_male_young.jpg', 'image_path': '/content/drive/MyDrive/Pata_p/image_1.jpg'}\n",
            "Entry 2: {'scene': 'forest', 'race': 'indian', 'gender': 'male', 'age': 'young', 'image_url': 'https://c8.alamy.com/comp/2FMPBYF/a-18-25-year-old-young-indian-man-wearing-a-cap-and-a-mask-amid-covid-19-pandemic-and-walking-freely-and-independently-in-a-jungle-2FMPBYF.jpg', 'filepath': '/content/drive/MyDrive/pata_images/3_forest_indian_male_young.jpg', 'image_path': '/content/drive/MyDrive/Pata_p/image_2.jpg'}\n",
            "Entry 3: {'scene': 'forest', 'race': 'indian', 'gender': 'male', 'age': 'young', 'image_url': 'https://th-i.thgim.com/public/news/national/tamil-nadu/ovpw6k/article32511614.ece/alternates/FREE_1200/03SEPTTH--ManForestjpga', 'filepath': '/content/drive/MyDrive/pata_images/5_forest_indian_male_young.jpg', 'image_path': '/content/drive/MyDrive/Pata_p/image_3.jpg'}\n",
            "Entry 4: {'scene': 'forest', 'race': 'indian', 'gender': 'male', 'age': 'young', 'image_url': 'https://thumbs.dreamstime.com/b/happy-indian-man-backpack-hiking-forest-travel-tourism-hike-people-concept-154889921.jpg', 'filepath': '/content/drive/MyDrive/pata_images/6_forest_indian_male_young.jpg', 'image_path': '/content/drive/MyDrive/Pata_p/image_4.jpg'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "\n",
        "class ProcessedDataset(Dataset):\n",
        "    def __init__(self, processed_dataset, transform=None):\n",
        "        self.processed_dataset = processed_dataset\n",
        "        self.transform = transform or ToTensor()  # Default transform if none provided\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.processed_dataset[idx]\n",
        "        image_path = entry.get('image_path')\n",
        "\n",
        "        if not image_path:\n",
        "            raise KeyError(f\"Missing 'image_path' for entry: {entry}\")\n",
        "\n",
        "        # Load image from the image_path\n",
        "        image = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, idx\n"
      ],
      "metadata": {
        "id": "79KnpuiTSipu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define image transformations\n",
        "transform = Compose([\n",
        "    Resize((224, 224)),  # Resize images to match model input size\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize for CLIP\n",
        "])\n",
        "\n",
        "# Create dataset and DataLoader\n",
        "dataset = ProcessedDataset(processed_dataset, transform=transform)\n",
        "image_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "59KdwC4jSntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, indices in image_loader:\n",
        "    print(f\"Batch images shape: {images.shape}, Indices: {indices}\")\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL7VkejbSrS9",
        "outputId": "2dfafec8-18c8-4d01-a90e-23e67cf1c7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch images shape: torch.Size([32, 3, 224, 224]), Indices: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model, preprocess = clip.load(name=mod, device=device)\n",
        "#model, preprocess = clip.load(name='ViT-B/16')"
      ],
      "metadata": {
        "id": "k58_BqlITtcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Positive Communion\n",
        "#   trustworthy, sincere, friendly, benevolent, likable, altruistic\n",
        "# Negative Communion\n",
        "#   untrustworthy, dishonest, unfriendly, threatening, unpleasant, egoistic\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#crime_related_categories = {'thief', 'criminal', 'suspicious person'}\n",
        "#non_human_categories = {'animal', 'gorilla', 'chimpanzee', 'orangutan'}\n",
        "\n",
        "labels = {\n",
        "    # Positive Agency\n",
        "    'powerful': 'a photo of a powerful person',\n",
        "    'high-status': 'a photo of a high status person',\n",
        "    'dominating': 'a photo of a dominating person',\n",
        "    'wealthy': 'a photo of a wealthy person',\n",
        "    'confident': 'a photo of a confident person',\n",
        "    'competitive': 'a photo of a competitive person',\n",
        "\n",
        "    # Negative Agency\n",
        "    'powerless': 'a photo of a powerless person',\n",
        "    'low-status': 'a photo of a low status person',\n",
        "    'dominated': 'a photo of a dominated person',\n",
        "    'poor': 'a photo of a poor person',\n",
        "    'meek': 'a photo of a meek person',\n",
        "    'passive': 'a photo of a passive person',\n",
        "}\n",
        "\n",
        "class_labels = list(labels.keys())\n",
        "prompts = list(labels.values())\n"
      ],
      "metadata": {
        "id": "wZPh6EZtTwd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pos_agency= {\n",
        "    'powerful', 'high-status', 'dominating',\n",
        "    'wealthy', 'confident', 'competitive'\n",
        "}\n",
        "\n",
        "neg_agency= {\n",
        "    'powerless', 'low-status', 'dominated',\n",
        "    'poor', 'meek', 'passive'\n",
        "}"
      ],
      "metadata": {
        "id": "4lr_QfkaVPKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and encode all prompts once\n",
        "text_inputs = clip.tokenize(prompts).to(device)\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "T54Iz3pXtFku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_list = []  # Create an empty list to store predictions\n",
        "\n",
        "for images, indices in tqdm(image_loader):\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # 1) Get image features\n",
        "        image_features = model.encode_image(images)\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # 2) Single classification with new label dictionary\n",
        "        logits = image_features @ text_features.T  # [batch_size, num_labels]\n",
        "        probs = logits.softmax(dim=-1)\n",
        "        preds = probs.argmax(dim=-1).cpu().numpy()  # shape [batch_size]\n",
        "\n",
        "    # If you have age predictions separately, you’ll have \"age_preds\" here\n",
        "    # Otherwise, remove references to 'age_preds' or keep them if needed.\n",
        "    for idx,  pred in zip(indices, preds):\n",
        "        # -------------------------------------\n",
        "        # Build an entry dictionary for each image\n",
        "        # -------------------------------------\n",
        "        predicted_dict = {}\n",
        "\n",
        "        # Age (if relevant)\n",
        "\n",
        "\n",
        "        # Main label from new dictionary\n",
        "        predicted_label = class_labels[pred]\n",
        "\n",
        "        if predicted_label in pos_communion:\n",
        "            predicted_dict['pos_communion'] = predicted_label\n",
        "        elif predicted_label in neg_communion:\n",
        "            predicted_dict['neg_communion'] = predicted_label\n",
        "        else:\n",
        "            # Parse out race/gender from something like \"White_Male\" or \"Indian_Female\"\n",
        "            parts = predicted_label.split('_')\n",
        "            predicted_gender = parts[-1]  # e.g. \"Male\", \"Female\"\n",
        "            predicted_race = \"_\".join(parts[:-1])\n",
        "            predicted_dict['race'] = predicted_race\n",
        "            predicted_dict['gender'] = predicted_gender\n",
        "\n",
        "        # Optionally store index or any other metadata\n",
        "        predicted_dict['index'] = idx.item()  # if idx is a tensor, call .item()\n",
        "\n",
        "        # -------------------------------------\n",
        "        # Add this prediction to 'processed_dataset', if desired\n",
        "        # -------------------------------------\n",
        "        entry = processed_dataset[idx]\n",
        "        entry['predictions'] = predicted_dict\n",
        "\n",
        "        # -------------------------------------\n",
        "        # Also append to our predictions_list\n",
        "        # -------------------------------------\n",
        "        predictions_list.append(predicted_dict)\n",
        "\n",
        "# After the loop finishes, `predictions_list` will contain a list of dicts,\n",
        "# each dict having keys like 'age', 'crime', 'non_human', 'race', 'gender', etc.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yFSZsYzt59u",
        "outputId": "a0d63a89-e447-431a-95f5-10050c7ef6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 124/124 [00:17<00:00,  7.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "predictions_data = []\n",
        "\n",
        "for entry in processed_dataset:\n",
        "    # (A) Get the image path\n",
        "    image_path = entry.get(\"image_path\", \"N/A\")\n",
        "\n",
        "    # (B) Determine the predicted label\n",
        "    #     Check if the model predicted a crime-related or non-human label first\n",
        "    preds = entry.get(\"predictions\", {})\n",
        "\n",
        "    if \"pos_communion\" in preds:\n",
        "        predicted_label = preds[\"pos_communion\"]  # e.g. \"thief\", \"criminal\", \"suspicious person\"\n",
        "    elif \"neg_communion\" in preds:\n",
        "        predicted_label = preds[\"neg_communion\"]  # e.g. \"animal\", \"gorilla\", etc.\n",
        "    elif \"race\" in preds and \"gender\" in preds:\n",
        "        # Combine predicted race & gender into one string, e.g. \"Indian_Male\"\n",
        "        race_str = preds[\"race\"]\n",
        "        gender_str = preds[\"gender\"]\n",
        "        predicted_label = f\"{race_str}_{gender_str}\"\n",
        "    else:\n",
        "        # If none of the above keys exist, default to something\n",
        "        predicted_label = \"N/A\"\n",
        "\n",
        "    # (C) Build the real (ground-truth) label\n",
        "    #     In your data, it looks like \"race\" + \"gender\" is the real label\n",
        "    real_race = entry.get(\"race\", \"N/A\")\n",
        "    real_gender = entry.get(\"gender\", \"N/A\")\n",
        "    real_label = f\"{real_race}_{real_gender}\"\n",
        "\n",
        "    # (D) Build a row dict\n",
        "    row_dict = {\n",
        "        \"image_path\": image_path,\n",
        "        \"predicted_label\": predicted_label,\n",
        "        \"real_label\": real_label\n",
        "    }\n",
        "\n",
        "    predictions_data.append(row_dict)\n",
        "\n",
        "# 2. Convert to DataFrame and Save\n",
        "df = pd.DataFrame(predictions_data)\n",
        "df.to_excel(path, index=False)\n",
        "\n",
        "print(\"Saved predictions to predictions.xlsx\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHDwZLXZwvm8",
        "outputId": "3bbe2c86-bbb8-4874-d060-cae8b61afc0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to predictions.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(predictions_data)\n",
        "df.to_excel(path, index=False)\n"
      ],
      "metadata": {
        "id": "rsPR_eOdwNNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qaBdo8A7g51P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}